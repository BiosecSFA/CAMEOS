import os
#If you don't want to use GPU, uncomment this line.
#os.environ["CUDA_VISIBLE_DEVICES"] = ""

import tensorflow as tf
import numpy as np

import random

def true_aa(seq):
	if "B" in seq or "J" in seq or "O" in seq or "U" in seq or "X" in seq or "Z" in seq:
		return False
	else:
		return True

def load_fasta(in_read, first_word = False, no_non_aa = False):
	seqs = {}
	cur_head = ""
	cur_seq = ""
	for line in in_read:
		if line[0] == ">":
			if cur_head != "":
				if not no_non_aa or true_aa(cur_seq):
					seqs[cur_head] = cur_seq
			cur_seq = ""
			if first_word:
				cur_head = line[1:].split(" ")[0].strip()
			else:
				cur_head = line[1:].strip()
		else:
			cur_seq += line.strip()
	if not no_non_aa or true_aa(cur_seq):
		seqs[cur_head] = cur_seq
	return seqs

def genes_to_array(gene_list):
	zero_mat = np.zeros([len(gene_list), len(gene_list[0])])
	#aa_to_int = {'.': 0, 'A': 1, 'R': 2, 'N': 3, 'D': 4, 'C': 5, 'Q': 6, 'E': 7, 'G': 8, 'H': 9, 'I': 10,
	#	'L': 11, 'K': 12, 'M': 13, 'F': 14, 'P': 15, 'S': 16, 'T': 17, 'W': 18, 'Y': 19, 'V': 20, '-': 21, 'X': 21, '*': 21}
	aa_to_int = {'.': 30, 'A': 0, 'R': 1, 'N': 2, 'D': 3, 'C': 4, 'Q': 5, 'E': 6, 'G': 7, 'H': 8, 'I': 9, 'L': 10, 'K': 11, 'M': 12, 'F': 13, 'P': 14, 'S': 15, 'T': 16, 'W': 17, 'Y': 18, 'V': 19, '-': 20, 'X': 20, '*': 20}
	numeric_seq = [[aa_to_int[aa] for aa in prot] for prot in gene_list]
	return np.asarray(numeric_seq)

def energy_calc(prot_mat, w1, w2):
	prot_mat = one_hot(prot_mat)
	skinny = tf.matmul(prot_mat, w1) #assuming w1 is shape (1, n_res).
	nprot_nres = tf.matmul(tf.matmul(prot_mat, w2), tf.transpose(prot_mat))
	final = tf.squeeze(skinny, [1]) + tf.linalg.diag_part(tf.matmul(tf.matmul(prot_mat, w2), tf.transpose(prot_mat)))
	return final

def psl_calc(prot_mat, w1, w2, nNodes):
	prot_mat = one_hot(prot_mat)
	nProt = prot_mat.get_shape()[0]

	pv_w2 = tf.matmul(prot_mat, w2)
	pv_w1 = tf.matmul(prot_mat, w1)

	#transposes: originate from porting julia into tensorflow code.
	partitions = tf.transpose(tf.reshape(tf.transpose(w1) + pv_w2, [nNodes * nProt, 21]))
	log_Zs = tf.reduce_sum(tf.transpose(tf.reshape(tf.reduce_logsumexp(tf.transpose(partitions), [1]), [nProt, nNodes])), [0])

	ull = tf.matmul(pv_w2, tf.transpose(prot_mat))
	numerator = tf.squeeze(pv_w1, [1]) + tf.linalg.diag_part(ull)
	all_scores = numerator - tf.transpose(log_Zs)
	return -all_scores

def one_hot(x):
	encoded = tf.one_hot(tf.cast(x, tf.int32), depth = 21, dtype = tf.float32)
	return tf.reshape(encoded, [x.shape[0], -1])

def psl_energy_calcs(core_name, gene_w1, gene_w2, msa_fa, batch_size = 64):

	tf.reset_default_graph()
	with tf.Session() as sess:

		w1 = np.load(gene_w1)
		w2 = np.load(gene_w2)

		align_fa = open(msa_fa)
		align_read = align_fa.readlines()
		align_fa.close()

		seqs = load_fasta(align_read)

		prot_len = len(seqs.values()[0])

		list_of_genes = [x for x in seqs.values()]

		gene_batches = filter(lambda n: len(n) == batch_size, [list_of_genes[i:i + batch_size] for i in range(0, len(list_of_genes), batch_size)])

		prot_batches = [genes_to_array(g_b) for g_b in gene_batches]

		#all_tf_prots = one_hot(prot_mat)
		tf_w1 = tf.cast(w1, tf.float32)
		tf_w2 = tf.cast(w2, tf.float32)
		tf_prots = tf.placeholder(tf.float32, shape=[batch_size, prot_len])

		energy_op = energy_calc(tf_prots, tf_w1, tf_w2)
		psl_op = psl_calc(tf_prots, tf_w1, tf_w2, prot_len)

		full_energies = []
		full_psls = []
		for prot_batch in prot_batches:
			new_energies = sess.run(energy_op, feed_dict={tf_prots: prot_batch})
			new_psls = sess.run(psl_op, feed_dict = {tf_prots: prot_batch})

			for n_e in new_energies:
				full_energies.append(n_e)
			for n_p in new_psls:
				full_psls.append(n_p)

		sess.close()

	return full_psls, full_energies

def main():
	gene_name = "aroB" #name of gene you are examining.
	gene_w1 = gene_name + "_w1.npy" #path to w1 npy file (generated by convert_jld_to_npy.jl)
	gene_w2 = gene_name + "_w2.npy" #path to w1 npy file (generated by convert_jld_to_npy.jl)
	gene_msa = gene_name + ".msa" #multiple sequence alignment we're looking at.
	#in all likelihood you won't be able to load all proteins into GPU memory.
	#batch_size is size of protein chunks loaded into memory. Should be large as possible w/o causing out-of-memory issues.
	batch_size = 64
	psls, energies = psl_energy_calcs(gene_name, gene_w1, gene_w2, "../main/msas/aroB.msa", batch_size = batch_size)
	#Output: will save many npy files as protein_window_60.npy with local energies across all genes.
	#The shape will be (number_of_positions, number_of_proteins). You can plot means/std along length to get similar output as our paper.

	out_file = open("../main/energies/energy_%s.txt" %(gene_name), "w")
	for energy in energies:
		out_file.write("%.3f\n" %(energy))
	out_file.close()

	out_file = open("../main/psls/psls_%s.txt" %(gene_name), "w")
	for psl in psls:
		out_file.write("%.3f\n" %(psl))
	out_file.close()

main()
